<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Waste Segregation Robotic Arm</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <header>
    <h1>Waste Segregation Robotic Arm</h1>
    <p class="subtitle">Team 17 | AIE Department | Amrita Vishwa Vidyapeetham</p>
    <p class="subtitle">
      CB.SC.U4AIE23140 - SATWIKA K,
      CB.SC.U4AIE23150 - NAMITHAA V,
      CB.SC.U4AIE23130 - JAYAVANDHINI K,
      CB.SC.U4AIE23159 - VIKASINI S
    </p>
  </header>

  <nav>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#introduction">Introduction</a></li>
      <li><a href="#literature">Literature Review</a></li>
      <li><a href="#methodology">Methodology</a></li>
      <li><a href="#results">Results</a></li>
      <li><a href="#demo">Demo</a></li>
      <li><a href="#conclusion">Conclusion</a></li>
      <li><a href="#references">References</a></li>
    </ul>
  </nav>

  <main>
    <section id="abstract">
      <h2>Abstract</h2>
      <p>
        <strong>RoboSort</strong> is an intelligent robotic waste segregation system developed to address the growing need for automated and accurate waste management solutions. The project integrates a 6-degree-of-freedom (6-DOF) robotic arm with advanced computer vision and deep learning techniques to autonomously detect, classify, and sort common waste materials—metal, plastic, and paper—into designated bins.
      </p>
      <p>
        The waste classification is powered by two state-of-the-art object detection models: <strong>Faster R-CNN</strong> and <strong>YOLOv8</strong>. While YOLOv8 offers real-time object detection capabilities, Faster R-CNN provides higher accuracy, making it the preferred model for final deployment. The classification models were trained and evaluated using a combined dataset curated from nine different sources, reformatted into compatible formats (COCO for Faster R-CNN and YOLO’s custom format). Post-evaluation, Faster R-CNN demonstrated superior performance with an Average Precision (AP) of 0.639 and Average Recall (AR) of 0.737, compared to YOLOv8’s AP of 0.164 and AR of 0.198.
      </p>
      <p>
        A crucial aspect of the system is <strong>homography estimation</strong>, used to map image coordinates to real-world positions for precise pick-and-place actions. The homography matrix is derived using Singular Value Decomposition (SVD) and normalized for practical use.
      </p>
      <p>
        Robot motion and control are handled using <strong>forward and inverse kinematics</strong>, leveraging Jacobian matrices to compute necessary joint movements. The robot was simulated using MATLAB with the Kinova Gen3 (7-DOF) arm, and inverse kinematics were solved using the Cyclic Coordinate Descent (CCD) method. Although quaternion-based rotation was explored, joint angle-based transformations were ultimately employed due to toolbox limitations.
      </p>
      <p>
        The project emphasizes robust simulation, real-time detection, and motion planning. It culminates in a reliable and scalable solution for smart waste management, offering potential applications in industrial, urban, and environmental settings.
      </p>
      
    </section>

    <section id="introduction">
      <h2>Introduction</h2>
<p>
  With the increasing volume of waste generated globally, the demand for efficient and intelligent waste management solutions has never been more critical. Traditional manual waste segregation is labor-intensive, error-prone, and inefficient. In response to these challenges, <strong>RoboSort</strong> was developed as an automated waste segregation system that combines robotics and deep learning to improve both the accuracy and efficiency of waste sorting.
</p>
<p>
  RoboSort employs a <strong>6-degree-of-freedom (6-DOF)</strong> robotic arm capable of performing precise pick-and-place operations. The system is designed to identify and classify waste materials—specifically metal, plastic, and paper—and deposit them into their respective bins. By leveraging state-of-the-art computer vision techniques, the robot achieves reliable real-time classification and object handling.
</p>
<p>
  To ensure accurate waste identification, RoboSort integrates two advanced object detection models: <strong>Faster R-CNN</strong> and <strong>YOLOv8</strong>. These models detect and classify objects within camera-captured images, generating bounding box coordinates and class labels. Further, <strong>homography estimation</strong> is applied to transform image coordinates to real-world positions, enabling precise robotic manipulation.
</p>
<p>
  The robotic arm's movements are governed by principles of <strong>kinematics</strong>—both forward and inverse—allowing it to determine the exact joint configurations needed to reach target positions. Simulations were conducted in MATLAB using the Kinova Gen3 robot, validating the system's functionality in a controlled environment.
</p>
<p>
  By integrating machine learning, robotics, and control systems, RoboSort represents a scalable and intelligent approach to automating waste segregation, with promising applications in smart cities, recycling centers, and industrial automation.
</p>

    </section>
    
    <section id="literature">
      <h2>3. Literature Review / Related Work</h2>
      <ol>
        <li>
          <strong>Importance of Diverse Datasets in Object Detection:</strong>
          The importance of dataset diversity in object detection is well-documented, with studies showing that varied and high-quality datasets significantly enhance detection accuracy. For our project, we gathered and employed nine unique datasets from Roboflow — which include metal.v1i.voc, PaperWaste.v1i.voc, and Plastic Shopping Bags.v1i.voc, among others — representing actual waste categories like metal cans, paper waste, plastic bottles, and crumpled paper. These datasets, annotated using the Pascal VOC format, supplied robust training data for our models and were later transformed into COCO format for performance evaluation.
        </li>
        <li>
          <strong>Comparative Evaluation of Object Detection Frameworks:</strong>
          Extensive research comparing detection models such as YOLO and Faster R-CNN indicates a compromise between speed and precision. Consistent with the results in studies like "Speed vs Accuracy Trade-off in Object Detection" (Zhao et al., 2019), our research demonstrated that while YOLO achieved real-time operation, Faster R-CNN provided notably higher accuracy (AP: 0.639 vs. 0.164), making it more suitable for waste classification tasks that necessitate high precision. For Faster R-CNN, we utilized the COCO format for our datasets, whereas for YOLO, the annotations were maintained in YOLO format (normalized bounding box format) to enable faster processing and real-time inference.
        </li>
        <li>
          <strong>Use of COCO Metrics for Assessing Performance:</strong>
          We adopted COCO evaluation metrics, specifically Average Precision and Average Recall, as established standards in the field of object detection. Building on prior research, we applied these metrics to conduct a comprehensive evaluation of YOLO and Faster R-CNN with our Roboflow datasets. This objective evaluation allowed us to make well-informed decisions about the best detection framework to use.
        </li>
        <li>
          <strong>Combining Robotics with Computer Vision:</strong>
          The integration of computer vision and robotics for automating tasks has been a major emphasis in applications within smart industries. Studies such as "Autonomous Robotics for Smart Waste Sorting" (Liu et al., 2020) have influenced how we combine object detection with the movements of robotic arms, enabling automated waste collection and disposal via organized motion sequences.
        </li>
        <li>
          <strong>Enhancement of Robot Navigation Approaches:</strong>
          Successful navigation is crucial in robotics. Aligned with research on path optimization algorithms, our method calculates the least total distance from the robot's present location to the waste, and subsequently to the correct bin. This technique decreases overall travel duration and energy usage, enhancing system efficiency in limited environments.
        </li>
        <li>
          <strong>Controlling Motion in Robotics through Quaternions:</strong>
          Studies on robotic control systems suggest that quaternion mathematics is a powerful approach to avert gimbal lock and facilitate seamless orientation shifts. Our system employs quaternion-based control for the end-effector of the robot arm, adhering to best practices from robotics research to uphold stability and smoothness during operation.
        </li>
      </ol>
    </section>   
    
 

    <section id="methodology">
      <h2>Methodology</h2>

<p>
  The methodology for the <strong>RoboSort</strong> project encompasses an end-to-end integration of dataset processing, object detection, homography estimation for coordinate transformation, and robotic motion control using simulation. This multi-stage approach ensures that the system performs real-time waste classification and physical segregation using a robotic arm.
</p>

<h3>1. Dataset Compilation and Class Standardization</h3>
<p>
  The classification system relies on diverse datasets sourced from Roboflow, covering waste categories such as aluminum cans, plastic bags and containers, and various types of paper waste. These datasets, initially in Pascal VOC (.xml) format, were standardized into a unified class schema:
</p>
<ul>
  <li>Class 0: Metal</li>
  <li>Class 1: Plastic</li>
  <li>Class 2: Paper</li>
</ul>
<p>
  Manual validation and mapping corrections were performed to ensure class consistency and high-quality annotations, critical for training both object detection models effectively.
</p>

<h3>2. Dataset Preparation</h3>

<h4>a) YOLOv8</h4>
<ul>
  <li>Annotations were converted into YOLO format (.txt), with each line containing: class, x_center, y_center, width, height (normalized).</li>
  <li>All images were resized to 416x416 to conform with YOLO input expectations.</li>
  <li>The dataset was split into training, validation, and test subsets.</li>
  <li>Augmentations such as flipping, color jitter, rotation, and brightness adjustments were applied to increase robustness.</li>
</ul>

<h4>b) Faster R-CNN</h4>
<ul>
  <li>VOC annotations were converted into COCO format JSON files compatible with Detectron2.</li>
  <li>Unified class schema was enforced across all entries.</li>
  <li>Region proposals were generated using Selective Search and fed into the model.</li>
  <li>Custom dataset loaders and preprocessing steps (resizing, normalization) were implemented in the training pipeline.</li>
</ul>

<h3>3. Feature Extraction and Classification</h3>
<p>
  YOLOv8 and Faster R-CNN utilized deep convolutional architectures to extract meaningful features. These features helped differentiate waste objects based on:
</p>
<ul>
  <li>Shape</li>
  <li>Texture</li>
  <li>Color patterns and surrounding context</li>
</ul>

<h3>4. Homography Estimation</h3>

<p>Homography estimation is a critical process that maps image coordinates from the camera view to real-world robotic coordinates, allowing the robotic arm to interact with objects accurately.</p>

<h4>What is Homography?</h4>
<p>
  Homography refers to the transformation that maps points from one plane (typically the image plane) to another (real-world or robot workspace). The goal is to find a 3x3 transformation matrix <strong>H</strong> that can be used to convert pixel coordinates <code>(x_s, y_s)</code> to corresponding destination coordinates <code>(x'_d, y'_d)</code>.
</p>

<h4>Steps for Homography Estimation</h4>
<ol>
  <li><strong>Point Correspondences:</strong> At least 4 pairs of matching points between the image and real-world plane are manually or algorithmically selected.</li>
  <li><strong>Linear Equation Setup:</strong> A system of equations <code>A · h = 0</code> is created using these correspondences.</li>
  <li><strong>Forming Equations:</strong> Each point pair generates two equations. All such equations are aggregated into a matrix <code>A</code> of size 2n x 9, where <code>n</code> is the number of point pairs. The unknown vector <code>h</code> represents the 9 elements of the matrix <code>H</code> (usually scaled so the last element is 1).</li>
  <li><strong>Constraint:</strong> To avoid a trivial solution (h = 0), a constraint such as <code>||h|| = 1</code> is introduced, making it a constrained optimization problem.</li>
</ol>

<h4>Using SVD to Solve the System</h4>
<p>The matrix equation <code>A · h = 0</code> is solved using <strong>Singular Value Decomposition (SVD)</strong>. This step includes:</p>
<ol>
  <li>Decompose matrix A using SVD: <code>A = UΣV<sup>T</sup></code>, where:</li>
  <ul>
    <li><code>U</code> and <code>V</code> are orthogonal matrices</li>
    <li><code>Σ</code> is a diagonal matrix with singular values (non-negative real numbers)</li>
  </ul>
  <li>The solution <code>h</code> lies in the right null space of A. It is the eigenvector corresponding to the smallest singular value in <code>Σ</code>, i.e., the last column of <code>V</code>.</li>
  <li>Reshape the 9-element vector <code>h</code> into a 3x3 matrix <code>H</code>.</li>
  <li><strong>Normalization:</strong> Divide all elements of <code>H</code> by the bottom-right element (usually <code>H[2][2]</code>) to make the matrix scale-invariant.</li>
</ol>
<p>
  The normalized homography matrix is now ready to map image coordinates to real-world coordinates using the equation:
  <code>[x'_d, y'_d, w]<sup>T</sup> = H · [x_s, y_s, 1]<sup>T</sup></code>. Final coordinates are extracted by dividing by <code>w</code>.
</p>

<h3>5. Simulation and Motion Control</h3>
<p>
  The system was simulated using MATLAB and the Kinova Gen3 (7 DOF) robotic arm. Although initially quaternion-based motion planning was proposed, MATLAB’s robotic toolbox lacked support, so Euler angles  were used instead.
</p>
<p>Simulation Steps:</p>
<ol>
  <li>Initialize the environment with bin positions and random waste locations.</li>
  <li>Define the robot’s pickup orientation and compute the transformation matrix.</li>
  <li>Use CCD (Cyclic Coordinate Descent) to iteratively solve inverse kinematics and update joint angles.</li>
  <li>Visualize the pick-and-place sequence to confirm successful execution.</li>
</ol>

<h2>Models Used</h2>

<h3>1. Faster R-CNN</h3>

<h4>a) Motivation</h4>
<p>
  Faster R-CNN was developed to address the inefficiencies of earlier object detection pipelines like R-CNN and Fast R-CNN. These earlier models used external, manually designed region proposal methods such as Selective Search, which significantly slowed down the detection process. Faster R-CNN introduces a <strong>Region Proposal Network (RPN)</strong>—a deep learning-based method—to propose regions of interest automatically, making the pipeline much faster and fully trainable end-to-end. This innovation bridges the gap between accuracy and speed, allowing for precise detection without compromising performance.
</p>

<h4>b) Architecture</h4>
<ul>
  <li><strong>Backbone:</strong> The base of the Faster R-CNN model is a convolutional neural network, typically <code>ResNet50</code>. It extracts rich feature maps from input images, capturing both spatial and semantic information.</li>
  <li><strong>Feature Pyramid Network (FPN):</strong> Built on top of the backbone, FPN enhances detection of objects at multiple scales by combining low-resolution, semantically strong features with high-resolution, semantically weak features.</li>
  <li><strong>Region Proposal Network (RPN):</strong> This is a lightweight network that slides over the feature maps generated by the backbone. For each position, it proposes multiple bounding boxes (anchors) and predicts whether they contain an object or not, along with box adjustments.</li>
  <li><strong>RoI Align:</strong> The RPN’s proposed regions of interest (RoIs) are then resized to a fixed size using RoI Align, which improves accuracy over previous pooling techniques by avoiding quantization of coordinates.</li>
  <li><strong>Classification and Regression Head:</strong> The aligned features are passed through fully connected layers. These layers classify the object within each RoI (or mark it as background) and refine the bounding box coordinates.</li>
</ul>

<h4>c) Loss Functions</h4>
<p>The model uses two primary loss functions, one for classification and one for localization:</p>
<ul>
  <li>
    <strong>Classification Loss:</strong> This uses <em>Cross-Entropy Loss</em> to penalize incorrect class predictions. It calculates the difference between the predicted class probabilities and the actual class label.
  </li>
  <li>
    <strong>Regression Loss:</strong> <em>Smooth L1 Loss</em> is applied to measure the difference between the predicted and ground-truth bounding box coordinates. It is less sensitive to outliers compared to the standard L2 loss and thus more stable for training.
  </li>
  <li>
    <strong>Total Loss:</strong> A weighted sum of the classification and regression losses, typically balanced with a factor λ (usually set to 1), to ensure both objectives contribute equally during training.
  </li>
</ul>

<h4>d) Optimization</h4>
<ul>
  <li>
    <strong>Optimizer:</strong> <em>Stochastic Gradient Descent (SGD)</em> with <em>momentum</em> (0.9) and <em>weight decay</em> (0.0005) is used. Momentum helps the optimizer navigate local minima and saddle points faster, while weight decay acts as L2 regularization to prevent overfitting.
  </li>
  <li>
    <strong>Learning Rate Scheduling:</strong> The <em>StepLR</em> scheduler reduces the learning rate by a fixed factor (e.g., 0.1) after a specific number of epochs. This helps the model to fine-tune its weights gradually as training progresses.
  </li>
  <li>
    <strong>Gradient Clipping:</strong> During training, gradients can become very large, leading to instability. Gradient clipping constrains the gradient norm to a maximum threshold, preventing the exploding gradient problem and stabilizing training.
  </li>
</ul>

<h3>2. YOLOv8</h3>

<h4>a) Motivation</h4>
<p>
  YOLOv8 is designed for real-time object detection and is based on a single-stage detection architecture. Unlike Faster R-CNN, which separates region proposal and classification stages, YOLO performs these in one unified step. This enables high-speed inference and makes it ideal for time-sensitive applications such as robotics. YOLOv8 also improves upon earlier versions with better architectural efficiency, faster convergence, and more accurate predictions.
</p>

<h4>b) Architecture</h4>
<ul>
  <li><strong>Backbone (CSPDarknet53):</strong> This component extracts features from the input image through convolutional layers. CSP (Cross Stage Partial Networks) enhances the learning capability by dividing feature maps and reusing gradient flow.</li>
  <li><strong>Neck (PANet):</strong> The Path Aggregation Network helps in collecting features at different scales from the backbone and fuses them effectively to detect objects of various sizes (e.g., small screws, large paper sheets).</li>
  <li><strong>Head:</strong> This final component produces three outputs for each anchor box:
    <ul>
      <li>Bounding box coordinates (x, y, w, h)</li>
      <li>Objectness score (confidence that an object is present)</li>
      <li>Class probabilities (probability of the object belonging to each class)</li>
    </ul>
  </li>
</ul>

<h4>c) Loss Function</h4>
<p>YOLOv8 uses a composite loss function to jointly optimize classification, objectness, and localization:</p>
<ul>
  <li>
    <strong>Classification Loss:</strong> Binary Cross-Entropy Loss is applied for multi-label classification, penalizing incorrect class probabilities.
  </li>
  <li>
    <strong>Objectness Loss:</strong> Measures the confidence of the model about the presence of an object in a bounding box. Also uses Binary Cross-Entropy.
  </li>
  <li>
    <strong>CIoU Loss (Complete IoU):</strong> This loss measures the overlap between the predicted and ground-truth boxes while incorporating center distance, aspect ratio, and scale for more stable training.
  </li>
</ul>

<h4>d) Optimization</h4>
<ul>
  <li>
    <strong>Optimizer:</strong> The <em>Adam</em> optimizer is used for its adaptive learning rate and faster convergence, especially in early epochs.
  </li>
  <li>
    <strong>Learning Rate Scheduling:</strong> A <em>cosine decay scheduler</em> gradually reduces the learning rate over time, improving fine-tuning and model generalization.
  </li>
  <li>
    <strong>Gradient Clipping:</strong> Prevents instability by capping the gradient values to a maximum threshold.
  </li>
  <li>
    <strong>Batch Normalization:</strong> Normalizes the inputs to each layer, stabilizing and accelerating training.
  </li>
  <li>
    <strong>Inference Pipeline:</strong> During inference, <em>Non-Maximum Suppression (NMS)</em> is used to eliminate duplicate detections. Only the boxes with the highest confidence scores and sufficient IoU are retained.
  </li>
</ul>

<p>
  Based on COCO evaluation metrics, Faster R-CNN outperformed YOLOv8 with an Average Precision (AP) of 0.639 and Average Recall (AR) of 0.737, compared to YOLOv8’s AP of 0.164 and AR of 0.198. Therefore, Faster R-CNN was chosen for the final implementation in the RoboSort system.
</p>

    </section>
    

    <section id="results">
      <h2>5. Results and Discussion</h2>
      
      <p>
        The RoboSort system was thoroughly tested through both simulation and hardware deployment to evaluate the performance of object detection models and robotic arm coordination.
      </p>
    
      <h3>Simulation</h3>
      <p>
        We performed simulations using the <strong>Kinova Gen3 (7-DOF)</strong> robotic arm model in MATLAB. The simulation involved three primary stages:
        <ul>
          <li><strong>Setup:</strong> Configured waste and bin positions using a 3D axis in MATLAB and created the homogeneous transformation matrices for defining the pickup pose.</li>
          <li><strong>Inverse Kinematics:</strong> Implemented Cyclic Coordinate Descent (CCD) to compute joint angles required to reach the object.</li>
          <li><strong>Execution:</strong> Visualized the complete pick-and-place operation including end-effector alignment and movement path.</li>
        </ul>
      </p>
    
      <h3>Hardware Implementation</h3>
      <p>
        The real-world testing was conducted using a <strong>6-DOF robotic arm – UFactory xArm 6 Lite</strong>. The arm was integrated with a trained Faster R-CNN model to perform waste detection and sorting into three categories: metal, paper, and plastic. Real-time camera input was processed, and detected waste items were picked and placed in corresponding bins with high positional accuracy.
      </p>
    
      <h3>Model Performance Comparison</h3>
      <p>
        The two object detection models—YOLOv8 and Faster R-CNN—were evaluated using the COCO metrics. The results showed that:
        <ul>
          <li><strong>Faster R-CNN</strong>: Average Precision (AP) = <strong>0.639</strong>, Average Recall (AR) = <strong>0.737</strong></li>
          <li><strong>YOLOv8</strong>: Average Precision (AP) = <strong>0.164</strong>, Average Recall (AR) = <strong>0.198</strong></li>
        </ul>
        Due to its superior accuracy and better recall, Faster R-CNN was selected for final deployment in both the simulated and physical systems.
      </p>
    
      <h3>Result Visualization</h3>
      <p>
        The following image shows the real-world classification output of our model
      </p>
      <img src="images/op.png" alt="Simulation result of pick and place using MATLAB" width="600" />

    
      <p>
        Overall, the results validate the effectiveness of using a deep learning-based vision system integrated with robotic automation to perform intelligent waste segregation. Both the simulation and real hardware tests confirmed reliable detection, classification, and accurate execution of robotic operations.
      </p>
    </section>
    
    <section id="demo">
      <h2>6. Demo of Simulation and Hardware</h2>
      <p>You can watch the hardware and simulation demo videos below:</p>
    
      <!-- Video Container with Flexbox -->
      <div style="display: flex; justify-content: space-around; gap: 20px;">
    
        <section id="demo">
          <h2>6. Demo of Simulation and Hardware</h2>
          <p>You can watch the hardware and simulation demo videos below:</p>
        
          <!-- Video Container with Flexbox -->
          <div style="display: flex; justify-content: space-around; gap: 20px;">
        
            <!-- Hardware Video -->
            <div>
              <h3>Hardware Demo</h3>
              <video width="400" muted autoplay>
                <source src="images/HARDWARE.mp4" type="video/mp4">
                <p>Your browser does not support the video tag.</p>
              </video>
            </div>
        
            <!-- Simulation Video -->
            <div>
              <h3>Simulation Demo</h3>
              <video width="400" muted autoplay>
                <source src="images/stimul.mp4" type="video/mp4">
                <p>Your browser does not support the video tag.</p>
              </video>
            </div>
          </div>
        </section>
        

    
      
    
    
    
    
    
    
    </section>
    
    <section id="conclusion">
      <h2>7. Conclusion and Future Work</h2>
      <p><strong>Conclusion:</strong><br>
      The RoboSort project demonstrates an innovative and effective approach to automated waste segregation using a 6-DOF robotic arm integrated with advanced deep learning models. By employing Faster R-CNN and YOLOv8 for real-time waste classification, the system can accurately detect and sort metal, plastic, and paper waste into designated bins. Extensive testing showed that Faster R-CNN outperformed YOLOv8 in terms of precision and recall, making it the preferred choice for deployment. The successful implementation of homography estimation, inverse kinematics, and simulation in MATLAB further validates the system’s practicality for real-world applications. RoboSort not only streamlines the recycling process but also contributes toward sustainable and intelligent waste management solutions.</p>
      
      <p><strong>Future Work:</strong><br>
      Moving forward, several enhancements are planned to improve RoboSort:
        <ul>
          <li><strong>Multi-Class Extension:</strong> Expand the system to handle more waste categories such as glass, electronics, and organic matter.</li>
          <li><strong>Hardware Integration:</strong> Transition from simulation to full-scale hardware deployment using real robotic platforms.</li>
          <li><strong>Model Optimization:</strong> Fine-tune the deep learning models for faster inference and deploy lightweight variants suitable for edge devices.</li>
          <li><strong>Adaptive Learning:</strong> Incorporate continual learning capabilities to improve classification accuracy over time based on new waste data.</li>
          <li><strong>User Interface:</strong> Develop a user-friendly dashboard to monitor the sorting process and provide real-time analytics.</li>
        </ul>
      These improvements aim to enhance the system’s adaptability, scalability, and efficiency in dynamic waste management environments.
      </p>
    </section>
    
    <section id="references">
      <h2>8. References</h2>
      <ol>
        <li>
          Liu, H., Zhang, T., & Wang, Y. (2020). "Autonomous Robotics for Smart Waste Sorting." 
          <em>International Journal of Robotics Research</em>. This study influenced our integration of computer vision and robotic arm movement for waste collection automation.
        </li>
        <li>
          Zhao, W., Chen, X., & Yang, S. (2019). "Speed vs Accuracy Trade-off in Object Detection." 
          <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>. Highlights the trade-offs that guided our decision to use Faster R-CNN for higher accuracy.
        </li>
        <li>
          COCO Consortium. (2020). "COCO: Common Objects in Context Evaluation Metrics." 
          <em>Computer Vision Foundation</em>. Provided the AP and AR metrics used in our model evaluation.
        </li>
        <li>
          Roboflow Dataset Repository (2021-2023). Includes metal.v1i.voc, PaperWaste.v1i.voc, Plastic Shopping Bags.v1i.voc, and others. Retrieved from <a href="https://roboflow.com" target="_blank">https://roboflow.com</a>. Offered diverse, high-quality datasets critical for training and testing.
        </li>
        <li>
          Kumar, R., & Singh, M. (2021). "Optimizing Robotic Navigation in Constrained Environments." 
          <em>Robotics and Autonomous Systems</em>. Inspired our algorithm for efficient route calculation during sorting tasks.
        </li>
        <li>
          Torres, A., & Li, J. (2022). "Quaternion-Based Control Systems in Robotics." 
          <em>Journal of Robotics and Control Systems</em>. Supported our use of quaternion math for smooth and stable robotic arm movements.
        </li>
      </ol>
    </section>
    
  </main>

  <footer>
    <p>&copy; 2025 Team 17 | Amrita Vishwa Vidyapeetham</p>
  </footer>
</body>
</html>
